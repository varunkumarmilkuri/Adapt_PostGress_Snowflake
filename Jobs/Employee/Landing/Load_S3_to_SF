import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from py4j.java_gateway import java_import

## @params: [JOB_NAME, URL, WAREHOUSE, DB, SCHEMA, USERNAME, PASSWORD]
SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
#args = getResolvedOptions(sys.argv, ['JOB_NAME', 'URL', 'WAREHOUSE', 'DB', 'SCHEMA', 'USERNAME', 'PASSWORD'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
logger = glueContext.get_logger()
java_import(spark._jvm, SNOWFLAKE_SOURCE_NAME)
## uj = sc._jvm.net.snowflake.spark.snowflake
spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())
sfOptions = {
"sfURL" : 'https://aggnvrr-lb09480.snowflakecomputing.com',
"sfUser" : 'adubey1',
"sfPassword" : 'MyPass1234@',
"sfDatabase" : 'ADAPT_TESTING',
"sfSchema" : 'ADAPT_SCHEMA',
"sfWarehouse" : 'COMPUTE_WH',
"application" : "AWSGlue"
}

## Read from a Snowflake table into a Spark Data Frame and writing to s3
## Without Line NO 32 it is failing with some wierd error. So just kept the line for the job to get completed
df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "employee").load()

logger.info("------------Reading S3 file into dataframe---------------")
inputdf = spark.read.option("header",True).option("inferSchema",True).option("multiLine",True).csv("s3://testadapt/test/")
inputdf.createOrReplaceTempView("employee")
inputdf=spark.sql("select * from employee")
logger.info("----------------read complete--------------")
if bool(inputdf.head(1)):
    inputdf.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "employee").mode("overwrite").save()

